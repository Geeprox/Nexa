const MOCK_REPLY_TEMPLATES = [
  "如果把你的问题拆成可执行步骤，我建议按\"定义目标 -> 建立比较维度 -> 设计验证样本 -> 形成复盘结论\"推进。先明确你真正要比较的是准确率、成本、速度还是可解释性，再把这些维度映射成可测指标。针对 {prompt} 这种任务，最稳妥的方式是先做一个小样本试运行，记录每次输出中的事实错误、结构缺失和风格漂移，再决定是否扩展到大样本。这样做的价值是可以在代价很低的时候提前暴露问题。",
  "你可以把这个问题当成一次\"轻量研究设计\"：先写假设，再做对照，再总结偏差来源。围绕 {prompt}，建议至少准备三组对照样本：高难度、常规难度和边界条件。每组都固定提示词结构，只改一个变量，最后做矩阵对比。输出时不要只看结论对不对，还要记录推理路径是否稳定、是否出现幻觉、是否需要过多后处理。这样后续接入真实模型 API 时，调试路径会非常清晰。",
  "从工程落地角度，我建议你先定义\"可接受结果\"，再决定提示词和模型策略。对于 {prompt}，可以先写一份最小验收标准：输出必须包含哪些字段、每个字段允许什么范围、什么情况下判定失败。然后让系统每次回答都走同一套结构化模板，持续积累失败样例。等样例足够后，再迭代提示词或模型分层策略。这样你不会陷入只凭主观感觉改 prompt 的循环，而是能用数据驱动优化。",
  "如果目标是提升回答质量并可复用，建议建立\"问题类型 -> 回复框架\"映射。像 {prompt} 这类问题，适合使用\"背景-方法-证据-风险-下一步\"五段式结构。先让模型输出框架，再要求它逐段补全，这比一次性长回答更稳定。你还可以给每段加上最小长度与引用要求，减少空洞结论。最终沉淀成模板后，后续同类问题可以直接复用，并且更容易评估改动效果。",
  "为了让系统具备可调试性，我建议你把一次对话拆成\"输入层、推理层、输出层\"三段日志。针对 {prompt}，输入层记录用户原文和预处理结果，推理层记录选择的策略分支，输出层记录最终回答和质量评分。即使现在还是 Mock 阶段，也可以先按这个格式沉淀。等接入真实 API 后，你就能快速定位是提示词问题、模型能力问题，还是后处理策略问题。",
  "可以先从\"失败优先\"的方式来推进：先假设这个方案会失败，再提前设计监控项。围绕 {prompt}，建议重点监控三类信号：事实错漏、结构缺段、上下文漂移。每次回答后自动给出这三项的自评结果，并把低分项写入复盘列表。这个机制会让你在功能看起来可用之前，就开始积累可操作的质量改进路径。",
  "如果你希望后续支持分叉式深挖，一个实用策略是把每次回答都拆成\"主结论 + 可分叉论点\"。在回答 {prompt} 时，可以先给一个总览结论，再明确列出 3 到 5 个可继续追问的子命题。这样用户在任意位置发起分叉时，系统更容易保留上下文一致性，也更便于把分支对话最终沉淀为结构化笔记。",
  "在当前阶段，最关键的是把交互闭环跑通，而不是追求模型真实性。对于 {prompt}，你可以先验证四件事：消息是否稳定上屏、流式过程是否平滑、重试是否可切换历史版本、分叉是否正确携带上下文。只要这四点稳定，后续接入真实 LLM API 基本不会改动交互主干，风险会显著下降。"
] as const;

export const INITIAL_CHAT_TURNS = [
  {
    user: "如何用 LLM 做文献对比？",
    assistant:
      "可以先把文献对比拆成固定字段：研究问题、数据来源、方法路线、实验设置、核心结论和局限。随后让模型按统一模板逐篇抽取，再用表格聚合。关键不在于一次回答很长，而在于字段定义稳定，这样你才能持续追加新文献并做横向比较。"
  },
  {
    user: "那如果不同论文指标口径不一致怎么办？",
    assistant:
      "先做指标归一化字典，把同义指标映射到统一维度，比如把不同命名的准确率、召回率映射到同一评价层。对无法直接映射的指标，要求模型输出\"原指标 + 解释 + 可比性等级\"。最终你会得到一个既能自动聚合又保留差异上下文的比较矩阵，避免把不可比数据硬合并。"
  }
] as const;

function renderTemplate(template: string, prompt: string) {
  return template.replaceAll("{prompt}", prompt.trim() || "该问题");
}

export function pickRandomMockReply(prompt: string, excludes: string[] = []) {
  const excluded = new Set(excludes.map((item) => item.trim()).filter(Boolean));
  const candidates = MOCK_REPLY_TEMPLATES.map((template) => renderTemplate(template, prompt)).filter(
    (candidate) => !excluded.has(candidate.trim())
  );

  const source =
    candidates.length > 0
      ? candidates
      : MOCK_REPLY_TEMPLATES.map((template) => renderTemplate(template, prompt));

  return source[Math.floor(Math.random() * source.length)];
}
